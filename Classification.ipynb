{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b74da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===========================\n",
    "# 1. Load Dataset\n",
    "# ===========================\n",
    "df = pd.read_excel(\"pos_charge_gravy.xlsx\")  # Ensure column \"Sequence\" exists\n",
    "sequences = df[\"Sequence\"].astype(str).tolist()\n",
    "existing_seqs = set(sequences)  # for fast lookup\n",
    "\n",
    "# ===========================\n",
    "# 2. Encode Amino Acids\n",
    "# ===========================\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"  # 20 standard aa\n",
    "aa_to_int = {aa: i+1 for i, aa in enumerate(amino_acids)}  # 0 reserved for padding\n",
    "int_to_aa = {i+1: aa for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "# Encode sequences as integers\n",
    "encoded = [[aa_to_int[aa] for aa in seq if aa in aa_to_int] for seq in sequences]\n",
    "\n",
    "# ===========================\n",
    "# 3. Prepare Training Data (Fixed Window Length = 20)\n",
    "# ===========================\n",
    "max_length = 20\n",
    "X, y = [], []\n",
    "\n",
    "for seq in encoded:\n",
    "    for i in range(1, len(seq)):\n",
    "        X.append(seq[:i])\n",
    "        y.append(seq[i])\n",
    "\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=max_length, padding=\"post\")\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=len(amino_acids)+1)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "print(f\"Training samples: {X_train.shape}, Validation samples: {X_val.shape}\")\n",
    "\n",
    "# ===========================\n",
    "# 4. Define LSTM Generative Model\n",
    "# ===========================\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=len(amino_acids)+1, output_dim=64, input_length=max_length),\n",
    "    tf.keras.layers.LSTM(128, return_sequences=False),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(len(amino_acids)+1, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "# ===========================\n",
    "# 5. Train Model with Early Stopping\n",
    "# ===========================\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ===========================\n",
    "# 6. Peptide Generation Functions\n",
    "# ===========================\n",
    "def sample_with_temperature(preds, temperature=1.0):\n",
    "    \"\"\"Sample next amino acid index with temperature scaling.\"\"\"\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds[0] = 0  # prevent padding token from being sampled\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.random.choice(len(preds), p=preds)\n",
    "\n",
    "def generate_peptide(max_len=20, temperature=1.0):\n",
    "    first_residue = np.random.choice(list(aa_to_int.values()))\n",
    "    seq = [first_residue]\n",
    "    \n",
    "    for _ in range(max_len - 1):\n",
    "        padded = tf.keras.preprocessing.sequence.pad_sequences([seq], maxlen=max_length, padding=\"post\")\n",
    "        pred = model.predict(padded, verbose=0)[0]\n",
    "        next_aa = sample_with_temperature(pred, temperature)\n",
    "        seq.append(next_aa)\n",
    "        if next_aa == 0:  # stop if padding token sampled\n",
    "            break\n",
    "    \n",
    "    return \"\".join(int_to_aa[i] for i in seq if i in int_to_aa)\n",
    "\n",
    "# ===========================\n",
    "# 7. Generate 3000 Unique Peptides\n",
    "# ===========================\n",
    "generated_sequences = set()  # use set to ensure uniqueness\n",
    "\n",
    "while len(generated_sequences) < 3000:\n",
    "    peptide = generate_peptide(max_len=20, temperature=1.0)\n",
    "    if len(peptide) >= 10 and peptide not in existing_seqs:\n",
    "        generated_sequences.add(peptide)\n",
    "\n",
    "# Save to CSV\n",
    "df_gen = pd.DataFrame({\"sequence\": sorted(generated_sequences)})\n",
    "output_file = \"generated_peptides_20.csv\"\n",
    "df_gen.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Generated {len(generated_sequences)} unique peptides saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10fd3d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Training classifier P1+N1 ...\n",
      "Reloading Tuner from tuner_results\\classifier_P1_N1\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91868\\Desktop\\python-workspace\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 128ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved model and results for classifier P1+N1\n",
      "\n",
      "[INFO] Training classifier P1+N2 ...\n",
      "Reloading Tuner from tuner_results\\classifier_P1_N2\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91868\\Desktop\\python-workspace\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 214ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved model and results for classifier P1+N2\n",
      "\n",
      "[INFO] Training classifier P2+N1 ...\n",
      "Reloading Tuner from tuner_results\\classifier_P2_N1\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91868\\Desktop\\python-workspace\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 118ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved model and results for classifier P2+N1\n",
      "\n",
      "[INFO] Training classifier P2+N2 ...\n",
      "Reloading Tuner from tuner_results\\classifier_P2_N2\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91868\\Desktop\\python-workspace\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 177ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved model and results for classifier P2+N2\n",
      "\n",
      "[INFO] Training classifier P3+N1 ...\n",
      "Reloading Tuner from tuner_results\\classifier_P3_N1\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91868\\Desktop\\python-workspace\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 155ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved model and results for classifier P3+N1\n",
      "\n",
      "[INFO] Training classifier P3+N2 ...\n",
      "Reloading Tuner from tuner_results\\classifier_P3_N2\\tuner0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\91868\\Desktop\\python-workspace\\.venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 192ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved model and results for classifier P3+N2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, roc_curve, precision_score, recall_score,\n",
    "    accuracy_score, f1_score, confusion_matrix, precision_recall_curve, auc\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# 1. Define datasets\n",
    "# ============================\n",
    "positive_files = [\n",
    "    \"pos_1 filtered_gravy.xlsx\",\n",
    "    \"pos_9 filtered_charge.xlsx\",\n",
    "    \"pos_filtered_charge_gravy.xlsx\"\n",
    "]\n",
    "\n",
    "negative_files = [\n",
    "    \"neg1_charge_gravy.xlsx\",\n",
    "    \"neg2_charge_gravy.xlsx\"\n",
    "]\n",
    "\n",
    "# ============================\n",
    "# 2. Encoding setup\n",
    "# ============================\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_int = {aa: i+1 for i, aa in enumerate(amino_acids)}  # 0 = padding\n",
    "num_tokens = len(amino_acids) + 1\n",
    "max_length = 30\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    return [aa_to_int[aa] for aa in seq if aa in aa_to_int]\n",
    "\n",
    "def preprocess_sequences(sequences):\n",
    "    encoded = [encode_sequence(seq) for seq in sequences]\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(encoded,\n",
    "                                                           maxlen=max_length,\n",
    "                                                           padding=\"post\")\n",
    "    return padded\n",
    "\n",
    "# ============================\n",
    "# 3. Augmentation function\n",
    "# ============================\n",
    "similar_groups = [\n",
    "    ['A','V','L','I'],   # aliphatic\n",
    "    ['F','W','Y'],       # aromatic\n",
    "    ['K','R','H'],       # positive\n",
    "    ['D','E'],           # negative\n",
    "    ['S','T','N','Q']    # polar\n",
    "]\n",
    "\n",
    "def mutate_seq(seq, p_mut=0.08):\n",
    "    seq = list(seq)\n",
    "    for i, aa in enumerate(seq):\n",
    "        if random.random() < p_mut:\n",
    "            for g in similar_groups:\n",
    "                if aa in g:\n",
    "                    choices = [x for x in g if x != aa]\n",
    "                    if choices:\n",
    "                        seq[i] = random.choice(choices)\n",
    "                    break\n",
    "    return \"\".join(seq)\n",
    "\n",
    "# ============================\n",
    "# 4. Attention Layer\n",
    "# ============================\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                                 initializer=\"random_normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        return tf.keras.backend.sum(x * a, axis=1)\n",
    "\n",
    "# ============================\n",
    "# 5. Model builder\n",
    "# ============================\n",
    "def build_model(hp):\n",
    "    inputs = tf.keras.layers.Input(shape=(max_length, num_tokens))\n",
    "    x = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(\n",
    "            units=hp.Int(\"units\", 64, 128, step=32),\n",
    "            return_sequences=True,\n",
    "            dropout=hp.Float(\"dropout\", 0.2, 0.4, step=0.1),\n",
    "            recurrent_dropout=0.2\n",
    "        )\n",
    "    )(inputs)\n",
    "    x = Attention()(x)\n",
    "    x = tf.keras.layers.Dropout(hp.Float(\"att_dropout\",0.2,0.4,step=0.1))(x)\n",
    "    x = tf.keras.layers.Dense(\n",
    "        hp.Int(\"dense_units\", 32, 64, step=32), activation=\"relu\"\n",
    "    )(x)\n",
    "    x = tf.keras.layers.Dropout(hp.Float(\"dense_dropout\",0.2,0.4,step=0.1))(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=hp.Choice(\"lr\",[0.001,0.0005])\n",
    "        ),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# ============================\n",
    "# 6. Training loop (all 6 classifiers)\n",
    "# ============================\n",
    "for pi, pos_file in enumerate(positive_files, start=1):\n",
    "    for ni, neg_file in enumerate(negative_files, start=1):\n",
    "        \n",
    "        print(f\"\\n[INFO] Training classifier P{pi}+N{ni} ...\")\n",
    "\n",
    "        # --- Set unique seeds per classifier\n",
    "        base_seed = 1000 + pi*10 + ni\n",
    "        tf.random.set_seed(base_seed)\n",
    "        np.random.seed(base_seed)\n",
    "        random.seed(base_seed)\n",
    "\n",
    "        # --- Load datasets\n",
    "        df_pos = pd.read_excel(pos_file)\n",
    "        df_neg = pd.read_excel(neg_file)\n",
    "\n",
    "        df_pos[\"label\"] = 1\n",
    "        df_neg[\"label\"] = 0\n",
    "\n",
    "        df_all = pd.concat([df_pos[[\"Sequence\",\"label\"]],\n",
    "                            df_neg[[\"Sequence\",\"label\"]]], ignore_index=True)\n",
    "\n",
    "        # --- Train-test split with unique seed\n",
    "        X = df_all[\"Sequence\"].values\n",
    "        y = df_all[\"label\"].values\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, stratify=y, random_state=base_seed, shuffle=True\n",
    "        )\n",
    "\n",
    "        # --- Augment positives\n",
    "        random.seed(base_seed)\n",
    "        augmented_pos = []\n",
    "        for seq in X_train[y_train==1]:\n",
    "            for _ in range(2):\n",
    "                augmented_pos.append(mutate_seq(seq))\n",
    "\n",
    "        X_train_aug = np.concatenate([X_train, augmented_pos])\n",
    "        y_train_aug = np.concatenate([y_train, np.ones(len(augmented_pos))])\n",
    "\n",
    "        # --- Encode sequences\n",
    "        X_train_enc = preprocess_sequences(X_train_aug)\n",
    "        X_test_enc = preprocess_sequences(X_test)\n",
    "        X_train_oh = tf.keras.utils.to_categorical(X_train_enc, num_classes=num_tokens)\n",
    "        X_test_oh = tf.keras.utils.to_categorical(X_test_enc, num_classes=num_tokens)\n",
    "\n",
    "        # --- Class weights\n",
    "        cw = class_weight.compute_class_weight(\n",
    "            \"balanced\", classes=np.unique(y_train_aug), y=y_train_aug\n",
    "        )\n",
    "        class_weights = dict(zip(np.unique(y_train_aug), cw))\n",
    "\n",
    "        # --- Hyperparameter tuning\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            build_model,\n",
    "            objective=\"val_accuracy\",\n",
    "            max_trials=10,\n",
    "            seed=base_seed,\n",
    "            directory=\"tuner_results\",\n",
    "            project_name=f\"classifier_P{pi}_N{ni}\"\n",
    "        )\n",
    "\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            X_train_oh, y_train_aug,\n",
    "            validation_split=0.2,\n",
    "            epochs=30,\n",
    "            batch_size=32,\n",
    "            class_weight=class_weights,\n",
    "            callbacks=[early_stop],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        # --- Retrieve best model\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "        # --- Evaluate\n",
    "        y_pred_probs = best_model.predict(X_test_oh).ravel()\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_probs)\n",
    "        pr, rc, _ = precision_recall_curve(y_test, y_pred_probs)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_probs)\n",
    "        pr_auc = auc(rc, pr)\n",
    "        optimal_threshold = thresholds[np.argmax(tpr - fpr)]\n",
    "        y_pred = (y_pred_probs >= optimal_threshold).astype(int)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        final_results = {\n",
    "            \"AUC\": auc_score,\n",
    "            \"PR-AUC\": pr_auc,\n",
    "            \"Optimal Threshold\": float(optimal_threshold),\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred),\n",
    "            \"Recall\": recall_score(y_test, y_pred),\n",
    "            \"F1\": f1_score(y_test, y_pred),\n",
    "            \"Sensitivity\": tp / (tp + fn),\n",
    "            \"Specificity\": tn / (tn + fp),\n",
    "            \"Confusion Matrix\": cm.tolist()\n",
    "        }\n",
    "\n",
    "        # --- Save results & model\n",
    "        base = f\"new_model_P{pi}_N{ni}\"\n",
    "        best_model.save(base + \".h5\")\n",
    "        joblib.dump(optimal_threshold, base + \"_threshold.pkl\")\n",
    "        pd.DataFrame([final_results]).to_csv(base + \"_results.csv\", index=False)\n",
    "\n",
    "        # --- Save ROC curve\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f\"AUC={auc_score:.3f}\")\n",
    "        plt.plot([0,1],[0,1],\"k--\")\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"ROC Curve P{pi}+N{ni}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(base + \"_roc.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        # --- Save PR curve\n",
    "        plt.figure()\n",
    "        plt.plot(rc, pr, label=f\"PR-AUC={pr_auc:.3f}\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"PR Curve P{pi}+N{ni}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(base + \"_pr.png\", dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        print(f\"[DONE] Saved model and results for classifier P{pi}+N{ni}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17c9a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3000 generated peptides from 'generated_peptides_20.csv'.\n",
      "Preprocessing sequences with max_length=30...\n",
      "Preprocessing complete. Data shape: (3000, 30, 21)\n",
      "\n",
      "--- Starting Classification ---\n",
      "\n",
      "Loading Model 1 from 'new_model_P1_N1.h5'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting with Model 1 (threshold = 0.8764217495918274)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Model 1: 1717/3000 (57.23%) classified as AMP\n",
      "  -> Results saved to 'thres new_20_results_model1.csv'\n",
      "\n",
      "Loading Model 2 from 'new_model_P1_N2.h5'...\n",
      "Predicting with Model 2 (threshold = 0.7175973653793335)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Model 2: 1848/3000 (61.60%) classified as AMP\n",
      "  -> Results saved to 'thres new_20_results_model2.csv'\n",
      "\n",
      "Loading Model 3 from 'new_model_P2_N1.h5'...\n",
      "Predicting with Model 3 (threshold = 0.8674263954162598)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Model 3: 1984/3000 (66.13%) classified as AMP\n",
      "  -> Results saved to 'thres new_20_results_model3.csv'\n",
      "\n",
      "Loading Model 4 from 'new_model_P2_N2.h5'...\n",
      "Predicting with Model 4 (threshold = 0.8103358745574951)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Model 4: 2019/3000 (67.30%) classified as AMP\n",
      "  -> Results saved to 'thres new_20_results_model4.csv'\n",
      "\n",
      "Loading Model 5 from 'new_model_P3_N1.h5'...\n",
      "Predicting with Model 5 (threshold = 0.36742377281188965)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Model 5: 2541/3000 (84.70%) classified as AMP\n",
      "  -> Results saved to 'thres new_20_results_model5.csv'\n",
      "\n",
      "Loading Model 6 from 'new_model_P3_N2.h5'...\n",
      "Predicting with Model 6 (threshold = 0.7637473940849304)...\n",
      "  -> Model 6: 2016/3000 (67.20%) classified as AMP\n",
      "  -> Results saved to 'thres new_20_results_model6.csv'\n",
      "\n",
      "Summary saved to 'thres new_20_classification_summary.csv'\n",
      "                 Model File      AMP % AMP Count Total Peptides Threshold Used\n",
      "Model 5  new_model_P3_N1.h5       84.7      2541           3000       0.367424\n",
      "Model 4  new_model_P2_N2.h5       67.3      2019           3000       0.810336\n",
      "Model 6  new_model_P3_N2.h5       67.2      2016           3000       0.763747\n",
      "Model 3  new_model_P2_N1.h5  66.133333      1984           3000       0.867426\n",
      "Model 2  new_model_P1_N2.h5       61.6      1848           3000       0.717597\n",
      "Model 1  new_model_P1_N1.h5  57.233333      1717           3000       0.876422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# ============================\n",
    "# 1. Define Custom Attention Layer (must match training code!)\n",
    "# ============================\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(\n",
    "            name=\"att_weight\",\n",
    "            shape=(input_shape[-1], 1),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            name=\"att_bias\",\n",
    "            shape=(input_shape[1], 1),\n",
    "            initializer=\"zeros\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n",
    "        a = tf.keras.backend.softmax(e, axis=1)\n",
    "        return tf.keras.backend.sum(x * a, axis=1)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 2. Optimal thresholds per model (STRICT)\n",
    "# ============================\n",
    "optimal_thresholds = {\n",
    "    1: 0.8764217495918274,  # Model 1\n",
    "    2: 0.7175973653793335,  # Model 2\n",
    "    3: 0.8674263954162598,  # Model 3\n",
    "    4: 0.8103358745574951,  # Model 4\n",
    "    5: 0.36742377281188965,  # Model 5\n",
    "    6: 0.7637473940849304,  # Model 6\n",
    "}\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 3. Load Generated Peptides\n",
    "# ============================\n",
    "gen_file = \"generated_peptides_20.csv\"\n",
    "if not os.path.exists(gen_file):\n",
    "    raise FileNotFoundError(f\"Error: Input file '{gen_file}' not found.\")\n",
    "\n",
    "df_gen = pd.read_csv(gen_file)\n",
    "\n",
    "# Normalize column names\n",
    "df_gen.columns = df_gen.columns.str.strip()\n",
    "\n",
    "if \"Sequence\" in df_gen.columns:\n",
    "    seq_col = \"Sequence\"\n",
    "elif \"sequence\" in df_gen.columns:\n",
    "    seq_col = \"sequence\"\n",
    "else:\n",
    "    raise KeyError(\n",
    "        \"Error: The CSV must contain a column named 'Sequence' or 'sequence'.\"\n",
    "    )\n",
    "\n",
    "sequences = df_gen[seq_col].astype(str).str.strip().tolist()\n",
    "print(f\"Loaded {len(sequences)} generated peptides from '{gen_file}'.\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 4. Encoding and Preprocessing\n",
    "# ============================\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_int = {aa: i + 1 for i, aa in enumerate(amino_acids)}  # 0 for padding\n",
    "num_tokens = len(amino_acids) + 1\n",
    "max_length = 30\n",
    "\n",
    "def encode_sequence(seq):\n",
    "    return [aa_to_int[aa] for aa in seq if aa in aa_to_int]\n",
    "\n",
    "def preprocess_sequences(seq_list):\n",
    "    encoded = [encode_sequence(seq) for seq in seq_list]\n",
    "    padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        encoded,\n",
    "        maxlen=max_length,\n",
    "        padding=\"post\",\n",
    "        truncating=\"post\",\n",
    "    )\n",
    "    one_hot = tf.keras.utils.to_categorical(\n",
    "        padded, num_classes=num_tokens\n",
    "    )\n",
    "    return one_hot\n",
    "\n",
    "print(f\"Preprocessing sequences with max_length={max_length}...\")\n",
    "X_gen = preprocess_sequences(sequences)\n",
    "print(f\"Preprocessing complete. Data shape: {X_gen.shape}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 5. Load Models and Classify\n",
    "# ============================\n",
    "model_files = [\n",
    "    \"new_model_P1_N1.h5\",\n",
    "    \"new_model_P1_N2.h5\",\n",
    "    \"new_model_P2_N1.h5\",\n",
    "    \"new_model_P2_N2.h5\",\n",
    "    \"new_model_P3_N1.h5\",\n",
    "    \"new_model_P3_N2.h5\",\n",
    "]\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "print(\"\\n--- Starting Classification ---\")\n",
    "\n",
    "for i, model_file in enumerate(model_files, start=1):\n",
    "    if not os.path.exists(model_file):\n",
    "        print(f\"WARNING: '{model_file}' not found. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    if i not in optimal_thresholds:\n",
    "        raise ValueError(f\"No optimal threshold defined for Model {i}\")\n",
    "\n",
    "    threshold = optimal_thresholds[i]\n",
    "\n",
    "    print(f\"\\nLoading Model {i} from '{model_file}'...\")\n",
    "    model = tf.keras.models.load_model(\n",
    "        model_file, custom_objects={\"Attention\": Attention}\n",
    "    )\n",
    "\n",
    "    print(f\"Predicting with Model {i} (threshold = {threshold})...\")\n",
    "    y_pred_probs = model.predict(X_gen, verbose=0).ravel()\n",
    "\n",
    "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
    "\n",
    "    amp_count = int(y_pred.sum())\n",
    "    total_count = len(y_pred)\n",
    "    perc_amp = 100 * amp_count / total_count\n",
    "\n",
    "    results_summary[f\"Model {i}\"] = {\n",
    "        \"Model File\": model_file,\n",
    "        \"AMP %\": perc_amp,\n",
    "        \"AMP Count\": amp_count,\n",
    "        \"Total Peptides\": total_count,\n",
    "        \"Threshold Used\": threshold,\n",
    "    }\n",
    "\n",
    "    # Save per-model predictions\n",
    "    df_out = df_gen.copy()\n",
    "    df_out[\"probability\"] = y_pred_probs\n",
    "    df_out[\"prediction\"] = y_pred\n",
    "\n",
    "    per_model_file = f\"thres new_20_results_model{i}.csv\"\n",
    "    df_out.to_csv(per_model_file, index=False)\n",
    "\n",
    "    print(\n",
    "        f\"  -> Model {i}: {amp_count}/{total_count} \"\n",
    "        f\"({perc_amp:.2f}%) classified as AMP\"\n",
    "    )\n",
    "    print(f\"  -> Results saved to '{per_model_file}'\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# 6. Save Summary\n",
    "# ============================\n",
    "if results_summary:\n",
    "    df_summary = (\n",
    "        pd.DataFrame(results_summary)\n",
    "        .T.sort_values(\"AMP %\", ascending=False)\n",
    "    )\n",
    "\n",
    "    summary_file = \"thres new_20_classification_summary.csv\"\n",
    "    df_summary.to_csv(summary_file, index=True)\n",
    "\n",
    "    print(f\"\\nSummary saved to '{summary_file}'\")\n",
    "    print(df_summary)\n",
    "else:\n",
    "    print(\"\\nNo models were processed. No summary created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "805ba625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total peptides loaded: 3000\n",
      "Columns: ['Sequence', 'probability', 'prediction']\n",
      "\n",
      "Peptides with prediction = 1: 2541\n",
      "Peptides with prediction = 0: 459\n",
      "\n",
      "Filtered peptides (prediction = 1):\n",
      "                Sequence  probability  prediction\n",
      "2   ADYWRIAKELRRYIRKVGRI     0.994740           1\n",
      "3   AECSGCICYWRRCRCCQVIK     0.948737           1\n",
      "5   AGAEEKIRQKLKNEIKKGRK     0.995973           1\n",
      "6   AGAKRIFNARRLKKIQEGKI     0.996008           1\n",
      "7   AGANRLTKELLEYLRKFGKI     0.984361           1\n",
      "8   AGANRLTKELLEYLRKFKKI     0.991034           1\n",
      "9   AGANRLTWELLKEYLRKRKK     0.992804           1\n",
      "10  AGANRLWLYLKEYLRKRGKK     0.993059           1\n",
      "11  AGAQRIWKELRRYIRKVGRI     0.995904           1\n",
      "12  AGAQRLKKKELYLRKRKGKI     0.997817           1\n",
      "\n",
      "‚úÖ Excel file saved: Model5_Predicted_Active_Peptides.xlsx\n",
      "Total active peptides in Excel: 2541\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the CSV file\n",
    "df = pd.read_csv('thres new_20_results_model5.csv')\n",
    "\n",
    "print(f\"Total peptides loaded: {len(df)}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Step 2: Filter for prediction = 1 only\n",
    "predicted_active = df[df['prediction'] == 1].copy()\n",
    "\n",
    "print(f\"\\nPeptides with prediction = 1: {len(predicted_active)}\")\n",
    "print(f\"Peptides with prediction = 0: {len(df[df['prediction'] == 0])}\")\n",
    "\n",
    "# Step 3: Display the filtered data\n",
    "print(\"\\nFiltered peptides (prediction = 1):\")\n",
    "print(predicted_active.head(10))\n",
    "\n",
    "# Step 4: Save to Excel file\n",
    "output_file = 'Model5_Predicted_Active_Peptides.xlsx'\n",
    "predicted_active.to_excel(output_file, index=False, sheet_name='Active Peptides')\n",
    "\n",
    "print(f\"\\n‚úÖ Excel file saved: {output_file}\")\n",
    "print(f\"Total active peptides in Excel: {len(predicted_active)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59645a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 2: Precision Filter with Models 4 & 6 ===\n",
      "Model 5 actives loaded: 2541 peptides\n",
      "Model 5 columns: ['Sequence', 'probability', 'prediction']\n",
      "Model 4 total: 3000 peptides\n",
      "Model 6 total: 3000 peptides\n",
      "Successfully merged: 2541 peptides (in all 3 models)\n",
      "\n",
      "‚úÖ STEP 2 COMPLETE!\n",
      "High-confidence candidates: 2049 peptides\n",
      "Avg Model 4 prob: 0.950\n",
      "Avg Model 6 prob: 0.937\n",
      "Avg Models 4+6 mean: 0.944\n",
      "\n",
      "üìä Excel saved: Step2_Models4_6_HighConf.xlsx\n",
      "\n",
      "Top 5 high-confidence candidates:\n",
      "                  Sequence  model4_prob  model6_prob  models46_mean\n",
      "1787  RKGKYYLFKKYIKKWKWKRK     0.999676     0.999433       0.999555\n",
      "1500  NRRQRWWKKLKKYIKKKWRK     0.999604     0.999498       0.999551\n",
      "1923  RWKKYIQLKKWYIKKKKGRK     0.999637     0.999414       0.999525\n",
      "2537  YWKKYQYLKKWWWEKKKGRK     0.999556     0.999482       0.999519\n",
      "1012  KKYRYYMKLWKIKKKKKGRK     0.999638     0.999361       0.999499\n",
      "\n",
      "üéØ NEXT: Step 3 - Weighted Ensemble Ranking!\n",
      "Use: Step2_Models4_6_HighConf.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== STEP 2: Precision Filter with Models 4 & 6 ===\")\n",
    "\n",
    "# Step 1: Load Model 5 active peptides (EXCEL file)\n",
    "df_model5 = pd.read_excel('Model5_Predicted_Active_Peptides.xlsx')\n",
    "print(f\"Model 5 actives loaded: {len(df_model5)} peptides\")\n",
    "print(f\"Model 5 columns: {df_model5.columns.tolist()}\")\n",
    "\n",
    "# Step 2: Load Model 4 and Model 6 prediction files (CSV)\n",
    "df_model4 = pd.read_csv('thres new_20_results_model4.csv')\n",
    "df_model6 = pd.read_csv('thres new_20_results_model6.csv')\n",
    "\n",
    "print(f\"Model 4 total: {len(df_model4)} peptides\")\n",
    "print(f\"Model 6 total: {len(df_model6)} peptides\")\n",
    "\n",
    "# Step 3: Merge all three datasets on 'Sequence' column\n",
    "df_merged = df_model5.merge(\n",
    "    df_model4[['Sequence', 'probability']], \n",
    "    left_on='Sequence', \n",
    "    right_on='Sequence', \n",
    "    suffixes=('', '_model4')\n",
    ")\n",
    "\n",
    "df_merged = df_merged.merge(\n",
    "    df_model6[['Sequence', 'probability']], \n",
    "    left_on='Sequence', \n",
    "    right_on='Sequence', \n",
    "    suffixes=('', '_model6')\n",
    ")\n",
    "\n",
    "# Rename probability columns for clarity\n",
    "df_merged = df_merged.rename(columns={\n",
    "    'probability_model4': 'model4_prob',\n",
    "    'probability_model6': 'model6_prob'\n",
    "})\n",
    "\n",
    "print(f\"Successfully merged: {len(df_merged)} peptides (in all 3 models)\")\n",
    "\n",
    "# Step 4: Calculate Models 4+6 mean probability\n",
    "df_merged['models46_mean'] = (df_merged['model4_prob'] + df_merged['model6_prob']) / 2\n",
    "\n",
    "# Step 5: Filter where Models 4+6 mean > 0.70 (high confidence)\n",
    "step2_final = df_merged[df_merged['models46_mean'] > 0.70].copy()\n",
    "step2_final = step2_final.sort_values('models46_mean', ascending=False)\n",
    "\n",
    "print(f\"\\n‚úÖ STEP 2 COMPLETE!\")\n",
    "print(f\"High-confidence candidates: {len(step2_final)} peptides\")\n",
    "print(f\"Avg Model 4 prob: {step2_final['model4_prob'].mean():.3f}\")\n",
    "print(f\"Avg Model 6 prob: {step2_final['model6_prob'].mean():.3f}\")\n",
    "print(f\"Avg Models 4+6 mean: {step2_final['models46_mean'].mean():.3f}\")\n",
    "\n",
    "# Step 6: Save results to Excel\n",
    "output_columns = ['Sequence', 'prediction', 'probability', 'model4_prob', 'model6_prob', 'models46_mean']\n",
    "step2_final[output_columns].to_excel('Step2_Models4_6_HighConf.xlsx', index=False)\n",
    "\n",
    "print(f\"\\nüìä Excel saved: Step2_Models4_6_HighConf.xlsx\")\n",
    "print(\"\\nTop 5 high-confidence candidates:\")\n",
    "print(step2_final[['Sequence', 'model4_prob', 'model6_prob', 'models46_mean']].head())\n",
    "\n",
    "print(\"\\nüéØ NEXT: Step 3 - Weighted Ensemble Ranking!\")\n",
    "print(\"Use: Step2_Models4_6_HighConf.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f795a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 3: Weighted Ensemble Ranking ===\n",
      "Step 2 high-confidence: 2049 peptides\n",
      "\n",
      "‚úÖ STEP 3 COMPLETE!\n",
      "Top 200 ensemble candidates: 200 peptides\n",
      "Ensemble score range: 0.998 - 0.999\n",
      "Avg ensemble score: 0.999\n",
      "\n",
      "üìä Excel saved: Step3_WeightedEnsemble_Top200.xlsx\n",
      "\n",
      "üèÜ TOP 5 ENSEMBLE CANDIDATES:\n",
      "               Sequence  model4_prob  model6_prob  ensemble_score\n",
      "2  RWKKYIQLKKWYIKKKKGRK     0.999637     0.999414        0.999483\n",
      "4  KKYRYYMKLWKIKKKKKGRK     0.999638     0.999361        0.999453\n",
      "0  RKGKYYLFKKYIKKWKWKRK     0.999676     0.999433        0.999446\n",
      "1  NRRQRWWKKLKKYIKKKWRK     0.999604     0.999498        0.999431\n",
      "9  HRKRWKIWRFLNKKKAKKIK     0.999626     0.999060        0.999375\n",
      "\n",
      "üéØ NEXT: Step 4 - Synthesis Filters!\n",
      "Use: Step3_WeightedEnsemble_Top200.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== STEP 3: Weighted Ensemble Ranking ===\")\n",
    "\n",
    "# Load Step 2 results\n",
    "df_step2 = pd.read_excel('Step2_Models4_6_HighConf.xlsx')\n",
    "print(f\"Step 2 high-confidence: {len(df_step2)} peptides\")\n",
    "\n",
    "# Calculate weighted ensemble score\n",
    "# Weights: 0.4√óModel4 + 0.4√óModel5 + 0.2√óModel6 (performance-based)\n",
    "df_step2['ensemble_score'] = (\n",
    "    0.4 * df_step2['model4_prob'] + \n",
    "    0.4 * df_step2['probability'] +  # Model 5 probability\n",
    "    0.2 * df_step2['model6_prob']\n",
    ")\n",
    "\n",
    "# Filter ensemble_score > 0.65 and take top 200\n",
    "step3_final = df_step2[df_step2['ensemble_score'] > 0.65].copy()\n",
    "step3_final = step3_final.sort_values('ensemble_score', ascending=False).head(200)\n",
    "\n",
    "print(f\"\\n‚úÖ STEP 3 COMPLETE!\")\n",
    "print(f\"Top 200 ensemble candidates: {len(step3_final)} peptides\")\n",
    "print(f\"Ensemble score range: {step3_final['ensemble_score'].min():.3f} - {step3_final['ensemble_score'].max():.3f}\")\n",
    "print(f\"Avg ensemble score: {step3_final['ensemble_score'].mean():.3f}\")\n",
    "\n",
    "# Save Step 3 results\n",
    "step3_final[['Sequence', 'probability', 'model4_prob', 'model6_prob', \n",
    "             'models46_mean', 'ensemble_score']].to_excel(\n",
    "    'Step3_WeightedEnsemble_Top200.xlsx', index=False\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Excel saved: Step3_WeightedEnsemble_Top200.xlsx\")\n",
    "print(\"\\nüèÜ TOP 5 ENSEMBLE CANDIDATES:\")\n",
    "print(step3_final[['Sequence', 'model4_prob', 'model6_prob', 'ensemble_score']].head())\n",
    "\n",
    "print(\"\\nüéØ NEXT: Step 4 - Synthesis Filters!\")\n",
    "print(\"Use: Step3_WeightedEnsemble_Top200.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3d8ac18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CALCULATE PEPTIDE PROPERTIES ===\n",
      "Loaded: 200 peptides\n",
      "Calculating properties...\n",
      "\n",
      "üìä PROPERTY SUMMARY:\n",
      "       net_charge       gravy  hydrophobic_moment  length\n",
      "count  200.000000  200.000000          200.000000   200.0\n",
      "mean     8.565000   -1.732700            0.807598    20.0\n",
      "std      2.089884    0.608548            0.383664     0.0\n",
      "min      5.000000   -3.385000            0.034811    20.0\n",
      "25%      7.000000   -2.040000            0.512857    20.0\n",
      "50%      8.000000   -1.800000            0.782574    20.0\n",
      "75%     10.000000   -1.561250            1.068653    20.0\n",
      "max     15.000000    0.475000            1.958485    20.0\n",
      "\n",
      "üî¨ TOP 5 PEPTIDES WITH PROPERTIES:\n",
      "               Sequence  net_charge  gravy  hydrophobic_moment  length  \\\n",
      "0  RWKKYIQLKKWYIKKKKGRK          11 -1.980            0.325065      20   \n",
      "1  KKYRYYMKLWKIKKKKKGRK          12 -2.150            0.200132      20   \n",
      "2  RKGKYYLFKKYIKKWKWKRK          11 -1.955            0.576620      20   \n",
      "3  NRRQRWWKKLKKYIKKKWRK          12 -2.595            1.280620      20   \n",
      "4  HRKRWKIWRFLNKKKAKKIK          12 -1.790            0.703824      20   \n",
      "\n",
      "   ensemble_score  \n",
      "0        0.999483  \n",
      "1        0.999453  \n",
      "2        0.999446  \n",
      "3        0.999431  \n",
      "4        0.999375  \n",
      "\n",
      "‚úÖ SAVED: Step3_Top200_WithProperties.xlsx\n",
      "All 200 peptides now have: net_charge, gravy, hydrophobic_moment, length\n",
      "\n",
      "üéØ READY FOR STEP 4 SYNTHESIS FILTERS!\n",
      "Use: Step3_Top200_WithProperties.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CALCULATE PEPTIDE PROPERTIES ===\")\n",
    "\n",
    "# Load Step 3 results\n",
    "df = pd.read_excel('Step3_WeightedEnsemble_Top200.xlsx')\n",
    "print(f\"Loaded: {len(df)} peptides\")\n",
    "\n",
    "# Amino acid hydrophobicity values for GRAVY (Kyte-Doolittle scale)\n",
    "hydrophobicity = {\n",
    "    'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "    'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "}\n",
    "\n",
    "# Function to calculate NET CHARGE (simple count at neutral pH)\n",
    "def calculate_net_charge(seq):\n",
    "    \"\"\"Net charge: +1 for R/K/H, -1 for D/E\"\"\"\n",
    "    seq = seq.upper()\n",
    "    positive = seq.count('R') + seq.count('K') + seq.count('H')\n",
    "    negative = seq.count('D') + seq.count('E')\n",
    "    return positive - negative\n",
    "\n",
    "# Function to calculate GRAVY (Grand Average of Hydropathicity)\n",
    "def calculate_gravy(seq):\n",
    "    \"\"\"GRAVY = average hydrophobicity per residue\"\"\"\n",
    "    seq = seq.upper()\n",
    "    n = len(seq)\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    gravy = sum(hydrophobicity.get(aa, 0) for aa in seq) / n\n",
    "    return gravy\n",
    "\n",
    "# Function to calculate HYDROPHOBIC MOMENT (MuH) - simplified Eisenberg\n",
    "def calculate_hydrophobic_moment(seq):\n",
    "    \"\"\"Simplified hydrophobic moment for amphipathicity\"\"\"\n",
    "    seq = seq.upper()\n",
    "    n = len(seq)\n",
    "    if n == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Angles (radians) for hydrophobic moment calculation\n",
    "    angle = np.pi * 100 / 180  # 100¬∞ angle between hydrophobic/hydrophilic\n",
    "    \n",
    "    sum_x = sum_y = 0\n",
    "    for i, aa in enumerate(seq):\n",
    "        h = hydrophobicity.get(aa, 0)\n",
    "        theta = i * angle\n",
    "        sum_x += h * np.cos(theta)\n",
    "        sum_y += h * np.sin(theta)\n",
    "    \n",
    "    muh = np.sqrt(sum_x**2 + sum_y**2) / n\n",
    "    return muh\n",
    "\n",
    "# Calculate ALL properties\n",
    "print(\"Calculating properties...\")\n",
    "df['length'] = df['Sequence'].str.len()\n",
    "df['net_charge'] = df['Sequence'].apply(calculate_net_charge)\n",
    "df['gravy'] = df['Sequence'].apply(calculate_gravy)\n",
    "df['hydrophobic_moment'] = df['Sequence'].apply(calculate_hydrophobic_moment)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nüìä PROPERTY SUMMARY:\")\n",
    "print(df[['net_charge', 'gravy', 'hydrophobic_moment', 'length']].describe())\n",
    "\n",
    "print(\"\\nüî¨ TOP 5 PEPTIDES WITH PROPERTIES:\")\n",
    "display_cols = ['Sequence', 'net_charge', 'gravy', 'hydrophobic_moment', 'length', 'ensemble_score']\n",
    "print(df[display_cols].head())\n",
    "\n",
    "# Save enhanced dataset with properties\n",
    "output_file = 'Step3_Top200_WithProperties.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ SAVED: Step3_Top200_WithProperties.xlsx\")\n",
    "print(f\"All 200 peptides now have: net_charge, gravy, hydrophobic_moment, length\")\n",
    "\n",
    "print(\"\\nüéØ READY FOR STEP 4 SYNTHESIS FILTERS!\")\n",
    "print(\"Use: Step3_Top200_WithProperties.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ea5b08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CALCULATE PEPTIDE PROPERTIES (EISENBERG UPDATE) ===\n",
      "Loaded: 200 peptides\n",
      "Calculating properties...\n",
      "\n",
      "üìä PROPERTY SUMMARY:\n",
      "       net_charge       gravy  hydrophobic_moment  length\n",
      "count  200.000000  200.000000          200.000000   200.0\n",
      "mean     8.565000   -1.732700            0.270899    20.0\n",
      "std      2.089884    0.608548            0.134275     0.0\n",
      "min      5.000000   -3.385000            0.016801    20.0\n",
      "25%      7.000000   -2.040000            0.164660    20.0\n",
      "50%      8.000000   -1.800000            0.251374    20.0\n",
      "75%     10.000000   -1.561250            0.338049    20.0\n",
      "max     15.000000    0.475000            0.685381    20.0\n",
      "\n",
      "‚úÖ SAVED: zzzz.xlsx\n",
      "Your Hydrophobic Moment is now correctly calculated using the Eisenberg Scale.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== CALCULATE PEPTIDE PROPERTIES (EISENBERG UPDATE) ===\")\n",
    "\n",
    "# Load Step 3 results\n",
    "df = pd.read_excel('Step3_WeightedEnsemble_Top200.xlsx')\n",
    "print(f\"Loaded: {len(df)} peptides\")\n",
    "\n",
    "# 1. Kyte-Doolittle Scale (Best for GRAVY)\n",
    "kd_scale = {\n",
    "    'A': 1.8, 'R': -4.5, 'N': -3.5, 'D': -3.5, 'C': 2.5,\n",
    "    'Q': -3.5, 'E': -3.5, 'G': -0.4, 'H': -3.2, 'I': 4.5,\n",
    "    'L': 3.8, 'K': -3.9, 'M': 1.9, 'F': 2.8, 'P': -1.6,\n",
    "    'S': -0.8, 'T': -0.7, 'W': -0.9, 'Y': -1.3, 'V': 4.2\n",
    "}\n",
    "\n",
    "# 2. Eisenberg Consensus Scale (Standard for Hydrophobic Moment)\n",
    "eisenberg_scale = {\n",
    "    'A': 0.62, 'R': -1.80, 'N': -0.78, 'D': -0.90, 'C': 0.29,\n",
    "    'Q': -0.85, 'E': -0.74, 'G': 0.48, 'H': -0.40, 'I': 1.38,\n",
    "    'L': 1.06, 'K': -1.50, 'M': 0.64, 'F': 1.19, 'P': 0.12,\n",
    "    'S': -0.18, 'T': -0.05, 'W': 0.81, 'Y': 0.26, 'V': 1.08\n",
    "}\n",
    "\n",
    "def calculate_net_charge(seq):\n",
    "    \"\"\"Net charge calculation at neutral pH\"\"\"\n",
    "    seq = str(seq).upper()\n",
    "    pos = seq.count('R') + seq.count('K') + seq.count('H')\n",
    "    neg = seq.count('D') + seq.count('E')\n",
    "    return pos - neg\n",
    "\n",
    "def calculate_gravy(seq):\n",
    "    \"\"\"GRAVY using Kyte-Doolittle scale\"\"\"\n",
    "    seq = str(seq).upper()\n",
    "    n = len(seq)\n",
    "    if n == 0: return 0\n",
    "    return sum(kd_scale.get(aa, 0) for aa in seq) / n\n",
    "\n",
    "def calculate_hydrophobic_moment(seq, angle_deg=100):\n",
    "    \"\"\"Hydrophobic moment using Eisenberg scale for alpha-helices\"\"\"\n",
    "    seq = str(seq).upper()\n",
    "    n = len(seq)\n",
    "    if n == 0: return 0\n",
    "    \n",
    "    angle_rad = np.deg2rad(angle_deg)\n",
    "    sum_x = 0\n",
    "    sum_y = 0\n",
    "    \n",
    "    for i, aa in enumerate(seq):\n",
    "        h = eisenberg_scale.get(aa, 0)\n",
    "        # Standard vector sum for residue positioning\n",
    "        theta = (i + 1) * angle_rad\n",
    "        sum_x += h * np.cos(theta)\n",
    "        sum_y += h * np.sin(theta)\n",
    "    \n",
    "    return np.sqrt(sum_x**2 + sum_y**2) / n\n",
    "\n",
    "# Calculate properties\n",
    "print(\"Calculating properties...\")\n",
    "df['length'] = df['Sequence'].str.len()\n",
    "df['net_charge'] = df['Sequence'].apply(calculate_net_charge)\n",
    "df['gravy'] = df['Sequence'].apply(calculate_gravy)\n",
    "df['hydrophobic_moment'] = df['Sequence'].apply(calculate_hydrophobic_moment)\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\nüìä PROPERTY SUMMARY:\")\n",
    "print(df[['net_charge', 'gravy', 'hydrophobic_moment', 'length']].describe())\n",
    "\n",
    "# Save enhanced dataset as zzzz\n",
    "output_file = 'zzzz.xlsx'\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ SAVED: {output_file}\")\n",
    "print(\"Your Hydrophobic Moment is now correctly calculated using the Eisenberg Scale.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aefed48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total peptides loaded: 200\n",
      "Peptides passing filters: 11\n",
      "‚úÖ Excel saved: memzzzz.xlsx\n",
      "‚úÖ FASTA saved: memzzzz.fasta\n",
      "\n",
      "üî¨ PREVIEW OF FILTERED PEPTIDES:\n",
      "                 Sequence  net_charge  gravy  hydrophobic_moment\n",
      "63   WGRESIKKLKKTEIKKWKKI           7 -1.455            0.434979\n",
      "73   NGWRKKLEKLKELYKWKKKI           7 -1.690            0.420112\n",
      "76   HARQTRIWKYLKKEIKKGKR           9 -1.790            0.487475\n",
      "113  KKYRDYYYTLPKKYIKWWIK           6 -1.460            0.406331\n",
      "126  QKKALYIRQYLKNEIKKGRK           7 -1.550            0.428125\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load the data\n",
    "input_file = 'zzzz.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "print(f\"Total peptides loaded: {len(df)}\")\n",
    "\n",
    "# 2. Apply the Synthesis Filters\n",
    "# Net Charge: +4 to +9\n",
    "# GRAVY: 0 to -1.8 (Note: 0 is the upper bound, -1.8 is the lower bound)\n",
    "# Hydrophobic Moment (muH): > 0.35\n",
    "filtered_df = df[\n",
    "    (df['net_charge'] >= 4) & (df['net_charge'] <= 9) &\n",
    "    (df['gravy'] <= 0) & (df['gravy'] >= -1.8) &\n",
    "    (df['hydrophobic_moment'] > 0.35)\n",
    "].copy()\n",
    "\n",
    "print(f\"Peptides passing filters: {len(filtered_df)}\")\n",
    "\n",
    "# 3. Save to Excel\n",
    "excel_output = 'memzzzz.xlsx'\n",
    "filtered_df.to_excel(excel_output, index=False)\n",
    "print(f\"‚úÖ Excel saved: {excel_output}\")\n",
    "\n",
    "# 4. Save to FASTA\n",
    "fasta_output = 'memzzzz.fasta'\n",
    "with open(fasta_output, 'w') as f:\n",
    "    for i, row in filtered_df.iterrows():\n",
    "        # Using index or a 'Sequence_ID' if available\n",
    "        seq_id = f\"Peptide_{i}_Charge{row['net_charge']}_uH{row['hydrophobic_moment']:.2f}\"\n",
    "        sequence = row['Sequence']\n",
    "        f.write(f\">{seq_id}\\n{sequence}\\n\")\n",
    "\n",
    "print(f\"‚úÖ FASTA saved: {fasta_output}\")\n",
    "\n",
    "# Display the filtered candidates\n",
    "if not filtered_df.empty:\n",
    "    print(\"\\nüî¨ PREVIEW OF FILTERED PEPTIDES:\")\n",
    "    print(filtered_df[['Sequence', 'net_charge', 'gravy', 'hydrophobic_moment']].head())\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No peptides matched those exact criteria. Consider loosening the GRAVY or uH constraints.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3e20571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Loading 6 model prediction files...\n",
      "  Model 1: 3000 peptides\n",
      "  Model 2: 3000 peptides\n",
      "  Model 3: 3000 peptides\n",
      "  Model 4: 3000 peptides\n",
      "  Model 5: 3000 peptides\n",
      "  Model 6: 3000 peptides\n",
      "\n",
      "üîó Merging all models...\n",
      "‚úÖ Merged: 3000 peptides in ALL models\n",
      "\n",
      "üìä Ensemble Mean Stats:\n",
      "Overall mean:  0.745\n",
      "Max mean:      0.998\n",
      "\n",
      "ü•á TOP 50 PEPTIDES SELECTED!\n",
      "#1 mean prob:  0.998\n",
      "#50 mean prob: 0.997\n",
      "\n",
      "üíæ SAVED:\n",
      "‚úÖ TOP_50_ENSEMBLE_MEAN_PEPTIDES.xlsx\n",
      "‚úÖ TOP_50_ENSEMBLE_MEAN_PEPTIDES.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SIMPLE Ensemble Mean Probability - Top 50 Peptides ONLY\n",
    "3-column format: Sequence,probability,prediction\n",
    "NO model-specific filters - Pure mean probability ranking\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ============================\n",
    "# 1. Load ALL 6 Model Files\n",
    "# ============================\n",
    "model_files = [\n",
    "    'thres new_20_results_model1.csv',\n",
    "    'thres new_20_results_model2.csv',\n",
    "    'thres new_20_results_model3.csv',\n",
    "    'thres new_20_results_model4.csv',\n",
    "    'thres new_20_results_model5.csv',\n",
    "    'thres new_20_results_model6.csv'\n",
    "]\n",
    "\n",
    "print(\"üî¨ Loading 6 model prediction files...\")\n",
    "dfs = []\n",
    "for i, file in enumerate(model_files, 1):\n",
    "    df = pd.read_csv(file)\n",
    "    df.columns = ['Sequence', 'probability', 'prediction']  # Force 3-column format\n",
    "    df['model_num'] = i\n",
    "    dfs.append(df[['Sequence', 'probability']])\n",
    "    print(f\"  Model {i}: {len(df)} peptides\")\n",
    "\n",
    "# ============================\n",
    "# 2. Merge on Sequence\n",
    "# ============================\n",
    "print(\"\\nüîó Merging all models...\")\n",
    "df_all = dfs[0].copy()\n",
    "df_all.columns = ['Sequence', 'model1_prob']\n",
    "\n",
    "for i, df in enumerate(dfs[1:], 2):\n",
    "    df_temp = df.copy()\n",
    "    df_temp.columns = ['Sequence', f'model{i}_prob']\n",
    "    df_all = df_all.merge(df_temp, on='Sequence', how='inner')\n",
    "\n",
    "print(f\"‚úÖ Merged: {len(df_all)} peptides in ALL models\")\n",
    "\n",
    "# ============================\n",
    "# 3. Calculate MEAN Probability (ALL 6 MODELS)\n",
    "# ============================\n",
    "model_prob_cols = ['model1_prob', 'model2_prob', 'model3_prob', \n",
    "                  'model4_prob', 'model5_prob', 'model6_prob']\n",
    "\n",
    "df_all['ensemble_mean_prob'] = df_all[model_prob_cols].mean(axis=1)\n",
    "\n",
    "print(f\"\\nüìä Ensemble Mean Stats:\")\n",
    "print(f\"Overall mean:  {df_all['ensemble_mean_prob'].mean():.3f}\")\n",
    "print(f\"Max mean:      {df_all['ensemble_mean_prob'].max():.3f}\")\n",
    "\n",
    "# ============================\n",
    "# 4. TOP 50 by Mean Probability\n",
    "# ============================\n",
    "top_50 = df_all.nlargest(50, 'ensemble_mean_prob')[['Sequence', 'ensemble_mean_prob'] + model_prob_cols]\n",
    "top_50 = top_50.sort_values('ensemble_mean_prob', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nü•á TOP 50 PEPTIDES SELECTED!\")\n",
    "print(f\"#1 mean prob:  {top_50['ensemble_mean_prob'].iloc[0]:.3f}\")\n",
    "print(f\"#50 mean prob: {top_50['ensemble_mean_prob'].iloc[49]:.3f}\")\n",
    "\n",
    "# ============================\n",
    "# 5. Save Results\n",
    "# ============================\n",
    "top_50.to_excel('TOP_50_ENSEMBLE_MEAN_PEPTIDES.xlsx', index=False)\n",
    "top_50.to_csv('TOP_50_ENSEMBLE_MEAN_PEPTIDES.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nüíæ SAVED:\")\n",
    "print(f\"‚úÖ TOP_50_ENSEMBLE_MEAN_PEPTIDES.xlsx\")\n",
    "print(f\"‚úÖ TOP_50_ENSEMBLE_MEAN_PEPTIDES.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9e9a71d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Loading TOP_50_ENSEMBLE_MEAN_PEPTIDES.csv...\n",
      "Loaded: 50 peptides\n",
      "Columns: ['Sequence', 'ensemble_mean_prob', 'model1_prob', 'model2_prob', 'model3_prob', 'model4_prob', 'model5_prob', 'model6_prob']\n",
      "\n",
      "‚úÖ SAVED: TOP_50_ENSEMBLE_MEAN_PEPTIDES.fasta\n",
      "  Contains 50 peptides\n",
      "\n",
      "Preview:\n",
      ">>Peptide_1_meanProb_0.998\n",
      "NRRQRWWKKLKKYIKKKWRK...\n",
      "‚úÖ SAVED: TOP_50_PEPTIDES_NUMBERED.fasta\n",
      "\n",
      "üéØ READY FOR SYNTHESIS ORDERS!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert TOP_50_ENSEMBLE_MEAN_PEPTIDES.csv ‚Üí FASTA format\n",
    "Simple: Sequence column ‚Üí >Peptide_1, >Peptide_2, etc.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ============================\n",
    "# Load Top 50 Results\n",
    "# ============================\n",
    "print(\"üî¨ Loading TOP_50_ENSEMBLE_MEAN_PEPTIDES.csv...\")\n",
    "df = pd.read_csv('TOP_50_ENSEMBLE_MEAN_PEPTIDES.csv')\n",
    "\n",
    "print(f\"Loaded: {len(df)} peptides\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# ============================\n",
    "# Convert to FASTA\n",
    "# ============================\n",
    "fasta_content = []\n",
    "for i, row in df.iterrows():\n",
    "    seq_name = f\"Peptide_{i+1}_meanProb_{row['ensemble_mean_prob']:.3f}\"\n",
    "    sequence = row['Sequence']\n",
    "    fasta_content.append(f\">{seq_name}\")\n",
    "    fasta_content.append(sequence)\n",
    "\n",
    "# ============================\n",
    "# Save FASTA\n",
    "# ============================\n",
    "with open('TOP_50_ENSEMBLE_MEAN_PEPTIDES.fasta', 'w') as f:\n",
    "    f.write('\\n'.join(fasta_content))\n",
    "\n",
    "print(f\"\\n‚úÖ SAVED: TOP_50_ENSEMBLE_MEAN_PEPTIDES.fasta\")\n",
    "print(f\"  Contains {len(df)} peptides\")\n",
    "print(f\"\\nPreview:\")\n",
    "print(\">\" + fasta_content[0])\n",
    "print(fasta_content[1][:60] + \"...\")\n",
    "\n",
    "# ============================\n",
    "# Also save numbered version (synthesis-friendly)\n",
    "# ============================\n",
    "with open('TOP_50_PEPTIDES_NUMBERED.fasta', 'w') as f:\n",
    "    for i, row in df.iterrows():\n",
    "        f.write(f\">{i+1}\\n\")\n",
    "        f.write(f\"{row['Sequence']}\\n\")\n",
    "\n",
    "print(f\"‚úÖ SAVED: TOP_50_PEPTIDES_NUMBERED.fasta\")\n",
    "print(\"\\nüéØ READY FOR SYNTHESIS ORDERS!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
